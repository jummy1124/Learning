{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet(nn.Module):\n",
    "    def __init__ (self,in_dim,n_hidden_1,n_hidden_2,out_dim):\n",
    "        super(simpleNet,self).__init__()\n",
    "        self.layer1=nn.Linear(in_dim,n_hidden_1)\n",
    "        self.layer2=nn.Linear(n_hidden_1,n_hidden_2)\n",
    "        self.layer3=nn.Linear(n_hidden_2,out_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        return x\n",
    "\n",
    "class Activation_Net(nn.Module):\n",
    "    def __init__ (self,in_dim,n_hidden_1,_hidden_2,out_dim):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.layer1=nn.Sequential(nn.Linear(in_dim,n_hidden_1),nn.ReLU(True))\n",
    "        self.layer2=nn.Sequential(nn.Linear(n_hidden_1,n_hidden_2),nn.ReLU(True))\n",
    "        self.layer3=nn.Sequential(nn.Linear(n_hidden_2,out_dim))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        return x\n",
    "    \n",
    "class Batch_Net(nn.Module):\n",
    "    def __init__ (self,in_dim,n_hidden_1,n_hidden_2,out_dim):\n",
    "        super(Batch_Net,self).__init__()\n",
    "        self.layer1=nn.Sequential(nn.Linear(in_dim,n_hidden_1),nn.BatchNorm1d(n_hidden_1),nn.ReLU(True))\n",
    "        self.layer2=nn.Sequential(nn.Linear(n_hidden_1,n_hidden_2),nn.BatchNorm1d(n_hidden_2),nn.ReLU(True))\n",
    "        self.layer3=nn.Sequential(nn.Linear(n_hidden_2,out_dim))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=slef.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 , loss : 1.3487 , acc : 67.7933\n",
      "epoch : 2 , loss : 0.5878 , acc : 84.4467\n",
      "epoch : 3 , loss : 0.4340 , acc : 87.3317\n",
      "epoch : 4 , loss : 0.3768 , acc : 88.5900\n",
      "epoch : 5 , loss : 0.3480 , acc : 89.2167\n",
      "epoch : 6 , loss : 0.3301 , acc : 89.7600\n",
      "epoch : 7 , loss : 0.3176 , acc : 90.1333\n",
      "epoch : 8 , loss : 0.3072 , acc : 90.4133\n",
      "epoch : 9 , loss : 0.3017 , acc : 90.6033\n",
      "epoch : 10 , loss : 0.2956 , acc : 90.7017\n",
      "epoch : 11 , loss : 0.2926 , acc : 91.0117\n",
      "epoch : 12 , loss : 0.2875 , acc : 90.9933\n",
      "epoch : 13 , loss : 0.2860 , acc : 91.1383\n",
      "epoch : 14 , loss : 0.2835 , acc : 91.2300\n",
      "epoch : 15 , loss : 0.2801 , acc : 91.3700\n",
      "epoch : 16 , loss : 0.2771 , acc : 91.4400\n",
      "epoch : 17 , loss : 0.2754 , acc : 91.5000\n",
      "epoch : 18 , loss : 0.2746 , acc : 91.5733\n",
      "epoch : 19 , loss : 0.2712 , acc : 91.6217\n",
      "epoch : 20 , loss : 0.2698 , acc : 91.6550\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "learning_rate=1e-2\n",
    "num_epoches=20\n",
    "\n",
    "data_tf=transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "train_dataset=datasets.MNIST(root='./data',train=True,transform=data_tf,download=True)\n",
    "test_dataset=datasets.MNIST(root='./data',train=False,transform=data_tf)\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "model=simpleNet(28*28,300,100,10)\n",
    "if torch.cuda.is_available():\n",
    "    model=model.cuda()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print_loss=0\n",
    "    if torch.cuda.is_available():\n",
    "        for data in train_loader:\n",
    "            img,label=data\n",
    "            img=img.view(img.size(0),-1)\n",
    "            if torch.cuda.is_available():\n",
    "                img=Variable(img,volatile=True).cuda()\n",
    "                label=Variable(label,volatile=True).cuda()\n",
    "            else:\n",
    "                img=Variable(img,volatile=True)\n",
    "                label=Variable(label,volatile=True)\n",
    "                \n",
    "            out=model(img)\n",
    "            loss=criterion(out,label)\n",
    "            print_loss+=loss.data.item()/label.size(0)\n",
    "            _, predict=torch.max(out.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predict == label).sum().item()\n",
    "            acc=100*correct / total\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print('epoch : {}'.format(epoch+1),', loss : {:.4f}'.format(print_loss),', acc : {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.2858,Acc : 0.9187\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss=0\n",
    "eval_acc=0\n",
    "for data in test_loader:\n",
    "    img,label=data\n",
    "    img=img.view(img.size(0),-1)\n",
    "    if torch.cuda.is_available():\n",
    "        img=Variable(img,volatile=True).cuda()\n",
    "        label=Variable(label,volatile=True).cuda()\n",
    "    else:\n",
    "        img=Variable(img,volatile=True)\n",
    "        label=Variable(label,volatile=True)\n",
    "    \n",
    "    out=model(img)\n",
    "    loss=criterion(out,label)\n",
    "    eval_loss+=loss.data.item()*label.size(0)\n",
    "    _,pred=torch.max(out,1)\n",
    "    num_correct=(pred==label).sum()\n",
    "    eval_acc+=num_correct.data.item()\n",
    "print('Test Loss : {:.4f},Acc : {:.4f}'.format(eval_loss/(len(test_dataset)),eval_acc/(len(test_dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
